worker_processes auto;   # 4 workers on a 4-CPU box

events {
    worker_connections 4096;  # Max FDs per worker ⇒ theoretical max ≈ 4 * 4096 connections
}

http {
    ##
    ## 1) Rate limiting per IP
    ##    30 requests/sec per IP, with a burst of 120 queued
    ##
    limit_req_zone $binary_remote_addr zone=req_per_ip:10m rate=30r/s;

    ##
    ## 2) Connection limiting (protect against too many open sockets)
    ##
    limit_conn_zone $binary_remote_addr zone=conn_per_ip:10m;
    limit_conn_zone $server_name        zone=conn_per_server:10m;

    upstream backend_api {
        server api:5000;
        keepalive 64;
    }

    server {
        listen 80;

        # Map Nginx's "I'm overloaded" 503 to 429 so you can see rate-limit vs real 503
        error_page 503 =429 /rate_limited;

        location / {
            #
            # Rate limiting (per IP)
            #
            # - 30 r/s steady
            # - burst 120 means Nginx will queue up to 120 extra before actually rejecting
            # - no 'nodelay' ⇒ excess gets delayed/queued instead of instantly failed
            #
            limit_req zone=req_per_ip burst=120;

            #
            # Connection limiting
            #
            # - up to 80 concurrent connections per single IP
            # - up to 1000 concurrent connections across the server
            #
            limit_conn conn_per_ip      80;
            limit_conn conn_per_server 1000;

            proxy_pass http://backend_api;
            proxy_http_version 1.1;
            proxy_set_header Connection "";
            proxy_set_header Host              $host;
            proxy_set_header X-Real-IP         $remote_addr;
            proxy_set_header X-Forwarded-For   $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        location = /rate_limited {
            return 429 "Too Many Requests\n";
        }
    }
}